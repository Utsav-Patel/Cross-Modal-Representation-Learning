{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47344791-b881-4433-82a9-fd496e3b082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import Recipe1MDataset\n",
    "from models import TextEncoder, ImageEncoder, CrossModalAttention\n",
    "from helper import calculate_metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64309b98-a1a5-4deb-90d7-ede45559b4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/common/home/as3503/.conda/envs/stylegan3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1248: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Change paths here.\n",
    "saved_model_path = '/common/home/as3503/as3503/courses/cs536/final_project/final_project/saved_models/model.pt'\n",
    "transformer_model_path = '/common/home/as3503/as3503/courses/cs536/final_project/final_project/saved_models/3shrex3f/model_train_encoders_False_epoch_1.pt'\n",
    "\n",
    "saved_weights = torch.load(saved_model_path, map_location='cpu')\n",
    "transformer_weights = torch.load(transformer_model_path, map_location='cpu')\n",
    "\n",
    "device = 'cuda:7'\n",
    "text_encoder = TextEncoder(2, 2)\n",
    "text_encoder.load_state_dict(saved_weights['txt_encoder'])\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "image_encoder.load_state_dict(saved_weights['img_encoder'])\n",
    "image_encoder = image_encoder.to(device)\n",
    "\n",
    "cm_transformer = CrossModalAttention().to(device)\n",
    "cm_transformer.load_state_dict(transformer_weights['cm_transformer'])\n",
    "cm_transformer = cm_transformer.to(device)\n",
    "\n",
    "val_dataset = Recipe1MDataset(part='val')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "batch_size = 8\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4d7a47-cce4-4c5e-a96a-a88f4bd20327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_transformer_input(image_features, text_embedding, input_attention_mask):\n",
    "    num_negative_to_positive_sample_ratio = 2\n",
    "\n",
    "    input_batch_size = image_features.shape[0]\n",
    "    output_batch_size = (num_negative_to_positive_sample_ratio + 1) * input_batch_size\n",
    "    ground_truths = torch.zeros(output_batch_size)\n",
    "    ground_truths[:input_batch_size] = 1\n",
    "\n",
    "    final_image_features = torch.zeros(output_batch_size, *image_features.shape[1:])\n",
    "    final_text_embeddings = torch.zeros(output_batch_size, *text_embedding.shape[1:])\n",
    "    output_attention_mask = torch.zeros(output_batch_size, *input_attention_mask.shape[1:])\n",
    "\n",
    "    final_image_features[:input_batch_size] = image_features\n",
    "    final_text_embeddings[:input_batch_size] = text_embedding\n",
    "\n",
    "    for run_num in range(num_negative_to_positive_sample_ratio):\n",
    "        a = torch.randperm(input_batch_size)\n",
    "        b = torch.zeros(input_batch_size).to(dtype=torch.int64)\n",
    "        for ind in range(input_batch_size):\n",
    "            c = random.randint(0, input_batch_size - 1)\n",
    "            while c == a[ind]:\n",
    "                c = random.randint(0, input_batch_size - 1)\n",
    "            b[ind] = c\n",
    "\n",
    "        final_image_features[(1 + run_num) * input_batch_size : (2 + run_num) * input_batch_size] = image_features[a]\n",
    "        final_text_embeddings[(1 + run_num) * input_batch_size : (2 + run_num) * input_batch_size] = text_embedding[b]\n",
    "        output_attention_mask[(1 + run_num) * input_batch_size : (2 + run_num) * input_batch_size] = \\\n",
    "            input_attention_mask[b]\n",
    "\n",
    "    return final_image_features, final_text_embeddings, output_attention_mask, ground_truths\n",
    "\n",
    "\n",
    "def save_model(model, fpath):\n",
    "    torch.save(model, fpath)\n",
    "\n",
    "\n",
    "def freeze_params(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def compute_ranks(sims):\n",
    "    # assert imgs.shape == rcps.shape, 'recipe features and image features should have same dimension'\n",
    "    # # pdb.set_trace()\n",
    "    # imgs = imgs / np.linalg.norm(imgs, axis=1)[:, None]\n",
    "    # rcps = rcps / np.linalg.norm(rcps, axis=1)[:, None]\n",
    "    # if retrieved_type == 'recipe':\n",
    "    #     sims = np.dot(imgs, rcps.T)  # [N, N]\n",
    "    # else:\n",
    "    #     sims = np.dot(rcps, imgs.T)\n",
    "\n",
    "    ranks = []\n",
    "    preds = []\n",
    "    # loop through the N similarities for images\n",
    "    for ii in range(sims.shape[0]):\n",
    "        # get a column of similarities for image ii\n",
    "        sim = sims[ii, :]\n",
    "        # sort indices in descending order\n",
    "        sorting = np.argsort(sim)[::-1].tolist()\n",
    "        # find where the index of the pair sample ended up in the sorting\n",
    "        pos = sorting.index(ii)\n",
    "        ranks.append(pos + 1.0)\n",
    "        preds.append(sorting[0])\n",
    "    # pdb.set_trace()\n",
    "    return np.asarray(ranks), preds\n",
    "\n",
    "\n",
    "def rank(rcps: list, imgs: list, attention_masks: list, model=None, retrieved_type='recipe', retrieved_range=100,\n",
    "         verbose=False, device='cuda'):\n",
    "\n",
    "    # save_model({'rcps': rcps, 'imgs': imgs, 'attention_masks': attention_masks}, 'data.pt')\n",
    "    # rcps = torch.cat(rcps, dim=0)\n",
    "    # imgs = torch.cat(imgs, dim=0)\n",
    "    # attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    t1 = time()\n",
    "    N = retrieved_range\n",
    "    data_size = len(imgs)\n",
    "    glob_rank = []\n",
    "    glob_recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    # pickler(imgs, 'image_outputs.pkl')\n",
    "    # pickler(rcps, 'recipe_outputs.pkl')\n",
    "    # if draw_hist:\n",
    "    #     plt.figure(figsize=(16, 6))\n",
    "    # average over 10 sets\n",
    "    for i in range(2):\n",
    "        ids_sub = np.random.choice(data_size, N, replace=False)\n",
    "        # imgs_sub = imgs[ids_sub, :]\n",
    "        # rcps_sub = rcps[ids_sub, :]\n",
    "        imgs_sub = [imgs[ind] for ind in ids_sub]\n",
    "        rcps_sub = [rcps[ind] for ind in ids_sub]\n",
    "        attention_masks_sub = [attention_masks[ind] for ind in ids_sub]\n",
    "        probs = np.zeros((N, N))\n",
    "        for x in tqdm(range(N)):\n",
    "            for y in range(N):\n",
    "                # if retrieved_type == 'recipe':\n",
    "                #     probs[x] = model(imgs_sub[x].repeat(N, 1, 1), rcps_sub)[:, 1]\n",
    "                # else:\n",
    "                #     probs[x] = model(imgs_sub, rcps_sub[x].repeat(N, 1, 1))[:, 1]\n",
    "                try:\n",
    "                    if retrieved_type == 'recipe':\n",
    "                        probs[x][y] = softmax(model(imgs_sub[x].unsqueeze(0).to(device), rcps_sub[y].unsqueeze(0).to(device),\n",
    "                                                    ~attention_masks_sub[y].bool().unsqueeze(0).to(device)))[0, 1]\n",
    "                    else:\n",
    "                        probs[x][y] = softmax(model(imgs_sub[y].unsqueeze(0).to(device), rcps_sub[x].unsqueeze(0).to(device),\n",
    "                                                    ~attention_masks_sub[y].bool().unsqueeze(0).to(device)))[0, 1]\n",
    "                except RuntimeError as e:\n",
    "                    print(imgs_sub[x].unsqueeze(0).shape, rcps_sub[y].unsqueeze(0).shape, attention_masks_sub[y].unsqueeze(0).shape)\n",
    "                    print(attention_masks_sub)\n",
    "                    print(ids_sub, x, y)\n",
    "                    raise(RuntimeError(str(e)))\n",
    "        # loop through the N similarities for images\n",
    "        ranks, _ = compute_ranks(probs)\n",
    "\n",
    "        recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "        for ii in recall.keys():\n",
    "            recall[ii] = (ranks <= ii).sum() / ranks.shape[0]\n",
    "        med = int(np.median(ranks))\n",
    "        for ii in recall.keys():\n",
    "            glob_recall[ii] += recall[ii]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i] / 10\n",
    "\n",
    "    medR = np.mean(glob_rank)\n",
    "    medR_std = np.std(glob_rank)\n",
    "    t2 = time()\n",
    "    if verbose:\n",
    "        print(f'=>retrieved_range={retrieved_range}, MedR={medR:.4f}({medR_std:.4f}), time={t2 - t1:.4f}s')\n",
    "        print(f'Global recall: 1: {glob_recall[1]:.4f}, 5: {glob_recall[5]:.4f}, 10: {glob_recall[10]:.4f}')\n",
    "    return medR, medR_std, glob_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "94e0609a-a8a6-4966-89a6-472958fa9b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1ff5b-0408-40b1-935f-01245d613c13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d256a7e5-a38e-475f-bce1-57934e51584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:01<00:00,  1.21s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:00<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>retrieved_range=100, MedR=51.0000(1.0000), time=241.9128s\n",
      "Global recall: 1: 0.0020, 5: 0.0130, 10: 0.0260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(51.0, 1.0, {1: 0.002, 5: 0.013000000000000001, 10: 0.026000000000000002})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(text_embeddings, image_features, attention_masks, model=cm_transformer, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317de68e-67f2-40eb-9795-f7c658e7b385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/common/home/as3503/as3503/courses/cs536/final_project/final_project\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af350113-e04e-487c-b89c-df076313b454",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_447881/1273944580.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/val_image_features.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/stylegan3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/stylegan3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "torch.save(image_features, 'saved_models/val_image_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d37bc75-67fb-42a8-b463-2a3ac66e445b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ac22d-481a-4631-96ab-7b9dbd00e7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a566d0c-2412-45f1-ba7a-508f1bc1f734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f338b76-ae84-47c4-82e8-315c6e48d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e7d1ba-8208-458e-8c69-a92d959f792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12148/12148 [1:17:56<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Calculating Metrics')\n",
    "image_encoder.eval()\n",
    "text_encoder.eval()\n",
    "cm_transformer.eval()\n",
    "\n",
    "text_embeddings = list()\n",
    "image_features = list()\n",
    "attention_masks = list()\n",
    "with torch.no_grad():\n",
    "    for text, image in tqdm(val_loader):\n",
    "        text_inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "        text_outputs = text_encoder(**text_inputs)\n",
    "        image_outputs = image_encoder(image.to(device))\n",
    "\n",
    "        for text_output, image_feature, attention_mask in zip(text_outputs, image_outputs, text_inputs.attention_mask):\n",
    "            text_embeddings.append(text_output.cpu())\n",
    "            image_features.append(image_feature.cpu())\n",
    "            attention_masks.append(attention_mask.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d7085-e9f4-4e03-99d2-256c25d86936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "stylegan3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47344791-b881-4433-82a9-fd496e3b082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Recipe1MDataset\n",
    "from time import time\n",
    "from torch import nn\n",
    "from models import TextEncoder, ImageEncoder\n",
    "from helper import calculate_metrics\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f96b82fa-3e12-4c72-9663-3be80facb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformer_input(image_features, text_embedding, input_attention_mask):\n",
    "    num_negative_to_positive_sample_ratio = 2\n",
    "\n",
    "    image_features = image_features.clone()\n",
    "    text_embedding = text_embedding.clone()\n",
    "    input_attention_mask = input_attention_mask.clone()\n",
    "\n",
    "    input_batch_size = image_features.shape[0]\n",
    "    output_batch_size = (num_negative_to_positive_sample_ratio + 1) * input_batch_size\n",
    "    ground_truths = torch.zeros(output_batch_size)\n",
    "    ground_truths[:input_batch_size] = 1\n",
    "\n",
    "    final_image_features = torch.zeros(output_batch_size, *image_features.shape[1:])\n",
    "    final_text_embeddings = torch.zeros(output_batch_size, *text_embedding.shape[1:])\n",
    "    output_attention_mask = torch.zeros(output_batch_size, *input_attention_mask.shape[1:])\n",
    "\n",
    "    final_image_features[:input_batch_size] = image_features.clone()\n",
    "    final_text_embeddings[:input_batch_size] = text_embedding.clone()\n",
    "\n",
    "    for run_num in range(num_negative_to_positive_sample_ratio):\n",
    "        a = torch.randperm(input_batch_size)\n",
    "        b = torch.zeros(input_batch_size).to(dtype=torch.int64)\n",
    "        for ind in range(input_batch_size):\n",
    "            c = random.randint(0, input_batch_size - 1)\n",
    "            while c == a[ind]:\n",
    "                c = random.randint(0, input_batch_size - 1)\n",
    "            b[ind] = c\n",
    "\n",
    "#         print(a)\n",
    "#         print(b)\n",
    "        \n",
    "        final_image_features[(1 + run_num) * input_batch_size : (2 + run_num) * input_batch_size] = image_features[a].clone()\n",
    "        final_text_embeddings[(1 + run_num) * input_batch_size : (2 + run_num) * input_batch_size] = text_embedding[b].clone()\n",
    "        output_attention_mask[(1 + run_num) * input_batch_size : (2 + run_num) * input_batch_size] = \\\n",
    "            input_attention_mask[b].clone()\n",
    "\n",
    "    return final_image_features.clone(), final_text_embeddings.clone(), output_attention_mask.clone(), ground_truths.clone()\n",
    "\n",
    "\n",
    "def save_model(model, fpath):\n",
    "    torch.save(model, fpath)\n",
    "\n",
    "\n",
    "def freeze_params(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "def compute_ranks(sims):\n",
    "    ranks = []\n",
    "    preds = []\n",
    "    # loop through the N similarities for images\n",
    "    for ii in range(sims.shape[0]):\n",
    "        # get a column of similarities for image ii\n",
    "        sim = sims[ii, :]\n",
    "        # sort indices in descending order\n",
    "        sorting = np.argsort(sim)[::-1].tolist()\n",
    "        # find where the index of the pair sample ended up in the sorting\n",
    "        pos = sorting.index(ii)\n",
    "        ranks.append(pos + 1.0)\n",
    "        preds.append(sorting[0])\n",
    "    # pdb.set_trace()\n",
    "    return np.asarray(ranks), preds\n",
    "\n",
    "\n",
    "def rank(rcps: list, imgs: list, attention_masks: list, model=None, retrieved_type='recipe', retrieved_range=100,\n",
    "         verbose=False, device='cuda'):\n",
    "    t1 = time()\n",
    "    N = retrieved_range\n",
    "    data_size = len(imgs)\n",
    "    glob_rank = []\n",
    "    glob_recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    # average over 10 sets\n",
    "    for i in range(2):\n",
    "        ids_sub = np.random.choice(data_size, N, replace=False)\n",
    "        # imgs_sub = imgs[ids_sub, :]\n",
    "        # rcps_sub = rcps[ids_sub, :]\n",
    "        imgs_sub = [imgs[ind] for ind in ids_sub]\n",
    "        rcps_sub = [rcps[ind] for ind in ids_sub]\n",
    "        attention_masks_sub = [attention_masks[ind] for ind in ids_sub]\n",
    "        probs = np.zeros((N, N))\n",
    "        for x in tqdm(range(N)):\n",
    "            for y in range(N):\n",
    "                # if retrieved_type == 'recipe':\n",
    "                #     probs[x] = model(imgs_sub[x].repeat(N, 1, 1), rcps_sub)[:, 1]\n",
    "                # else:\n",
    "                #     probs[x] = model(imgs_sub, rcps_sub[x].repeat(N, 1, 1))[:, 1]\n",
    "                try:\n",
    "                    if retrieved_type == 'recipe':\n",
    "                        probs[x][y] = softmax(model(imgs_sub[x].unsqueeze(0).to(device), rcps_sub[y].unsqueeze(0).to(device),\n",
    "                                                    ~attention_masks_sub[y].bool().unsqueeze(0).to(device)))[0, 1]\n",
    "                    else:\n",
    "                        probs[x][y] = softmax(model(imgs_sub[y].unsqueeze(0).to(device), rcps_sub[x].unsqueeze(0).to(device),\n",
    "                                                    ~attention_masks_sub[y].bool().unsqueeze(0).to(device)))[0, 1]\n",
    "                except RuntimeError as e:\n",
    "                    print(imgs_sub[x].unsqueeze(0).shape, rcps_sub[y].unsqueeze(0).shape, attention_masks_sub[y].unsqueeze(0).shape)\n",
    "                    print(attention_masks_sub)\n",
    "                    print(ids_sub, x, y)\n",
    "                    raise(RuntimeError(str(e)))\n",
    "        # loop through the N similarities for images\n",
    "        ranks, _ = compute_ranks(probs)\n",
    "\n",
    "        recall = {1: 0.0, 5: 0.0, 10: 0.0}\n",
    "        for ii in recall.keys():\n",
    "            recall[ii] = (ranks <= ii).sum() / ranks.shape[0]\n",
    "        med = int(np.median(ranks))\n",
    "        for ii in recall.keys():\n",
    "            glob_recall[ii] += recall[ii]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i] / 10\n",
    "\n",
    "    medR = np.mean(glob_rank)\n",
    "    medR_std = np.std(glob_rank)\n",
    "    t2 = time()\n",
    "    if verbose:\n",
    "        print(f'=>retrieved_range={retrieved_range}, MedR={medR:.4f}({medR_std:.4f}), time={t2 - t1:.4f}s')\n",
    "        print(f'Global recall: 1: {glob_recall[1]:.4f}, 5: {glob_recall[5]:.4f}, 10: {glob_recall[10]:.4f}')\n",
    "    return medR, medR_std, glob_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1d28596-76de-4b0f-a915-c02bd8ad410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, n_heads=2, n_layers=2, num_image_patches=197, num_classes=2, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.text_pos_embed = SinusoidalPositionalEncoding(model_dim, dropout=drop_rate)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        self.image_pos_embed = nn.Parameter(torch.zeros(1, num_image_patches + 1, model_dim))\n",
    "        self.image_pos_drop = nn.Dropout(p=drop_rate)\n",
    "        layers = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "        \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        batch_size = image_features.shape[0]\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        image_features = torch.cat((cls_token, image_features), dim=1)\n",
    "        image_features = image_features + self.image_pos_embed\n",
    "        image_features = self.image_pos_drop(image_features)\n",
    "        \n",
    "        text_features = self.text_pos_embed(text_features)\n",
    "        \n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(batch_size, image_features.shape[1] + 1).to(transformer_input.device), src_key_padding_mask), dim=1)\n",
    "        transformer_input = transformer_input.transpose(1, 0)\n",
    "        src_key_padding_mask = src_key_padding_mask.transpose(1, 0)\n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\n",
    "        cls_outputs = transformer_outputs[0, :, :]\n",
    "        return self.cls_projection(cls_outputs)\n",
    "        # return transformer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0d5f1b-5087-4ad5-9480-9282a5809fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/common/home/as3503/.conda/envs/stylegan3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:1248: UserWarning: Argument resample is deprecated and will be removed since v0.10.0. Please, use interpolation instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Change paths here.\n",
    "saved_model_path = '/common/home/as3503/as3503/courses/cs536/final_project/final_project/saved_models/model.pt'\n",
    "transformer_model_path = '/common/home/as3503/as3503/courses/cs536/final_project/final_project/saved_models/1b1huuko/model_train_encoders_False_epoch_0.pt'\n",
    "\n",
    "saved_weights = torch.load(saved_model_path, map_location='cpu')\n",
    "# transformer_weights = torch.load(transformer_model_path, map_location='cpu')\n",
    "\n",
    "device = 'cuda:7'\n",
    "text_encoder = TextEncoder(2, 2)\n",
    "text_encoder.load_state_dict(saved_weights['txt_encoder'])\n",
    "text_encoder = text_encoder.to(device)\n",
    "\n",
    "image_encoder = ImageEncoder()\n",
    "image_encoder.load_state_dict(saved_weights['img_encoder'])\n",
    "image_encoder = image_encoder.to(device)\n",
    "\n",
    "cm_transformer = CrossModalAttention().to(device)\n",
    "# cm_transformer.load_state_dict(transformer_weights['cm_transformer'])\n",
    "cm_transformer = cm_transformer.to(device)\n",
    "\n",
    "val_dataset = Recipe1MDataset(part='val')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "batch_size = 8\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba8d40f1-3c31-4270-8c3c-2343e92449af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_transformer = CrossModalAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "17298345-c02c-4443-934c-0929a922c956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_transformer.load_state_dict(torch.load('saved_models/k89llnjz/model_train_encoders_False_num_its_1000.pt', map_location='cpu')['cm_transformer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9acf3992-65f5-4cba-93bb-d2d36dbb3f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_encoder = text_encoder.to(device)\n",
    "# image_encoder = image_encoder.to(device)\n",
    "# cm_transformer = cm_transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fbddf054-8f69-4353-8e98-4d254de86a81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06]], device='cuda:7')\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "cm_transformer.eval()\n",
    "with torch.no_grad():\n",
    "    for text, image in val_loader:\n",
    "        text_inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "        text_outputs = text_encoder(**text_inputs)\n",
    "        image_outputs = image_encoder(image.to(device))\n",
    "        # outputs = []\n",
    "        # for i in range(8):\n",
    "        #     for j in range(8):\n",
    "        #         output = cm_transformer(image_outputs[i].unsqueeze(0), text_outputs[j].unsqueeze(0), ~text_inputs.attention_mask[j].bool().unsqueeze(0))\n",
    "                # outputs.append(torch.softmax(output, dim=1).detach().cpu())\n",
    "        transformer_image_inputs, transformer_text_inputs, output_attention_mask, ground_truth = \\\n",
    "            get_transformer_input(image_outputs, text_outputs, text_inputs.attention_mask)\n",
    "        text_padding_mask = ~output_attention_mask.bool()\n",
    "        indices = torch.randperm(transformer_image_inputs.size()[0])\n",
    "        outputs = cm_transformer(transformer_image_inputs[indices].to(device), transformer_text_inputs[indices].to(device), text_padding_mask[indices].to(device))\n",
    "        # print(transformer_image_inputs)\n",
    "        # print(transformer_text_inputs)\n",
    "        # output1 = cm_transformer(transformer_image_inputs[1, :, :].unsqueeze(0).to(device), transformer_text_inputs[6, :, :].unsqueeze(0).to(device), text_padding_mask[6, :].unsqueeze(0).to(device))\n",
    "        # output2 = cm_transformer(transformer_image_inputs[10].unsqueeze(0).to(device), transformer_text_inputs[10].unsqueeze(0).to(device), text_padding_mask[10].unsqueeze(0).to(device))\n",
    "        # outputs = cm_transformer(image_outputs[6].unsqueeze(0), text_outputs[1].unsqueeze(0), ~text_inputs.attention_mask[1].bool().unsqueeze(0))\n",
    "        # print(outputs)\n",
    "        print(torch.softmax(outputs, dim=1))\n",
    "        print(ground_truth)\n",
    "        # pprint(output1)\n",
    "        # pprint(output2)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6222dc4-3b18-4660-996b-47afaca4086f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "fbe01b36-522f-4657-926e-4c377c7bbe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06],\n",
      "        [9.9999e-01, 9.6588e-06]], device='cuda:7')\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(8, 197, 768)\n",
    "b = torch.randn(8, 200, 768)\n",
    "# c = torch.randn(8, 200).bool()\n",
    "c = torch.zeros(8, 200)\n",
    "with torch.no_grad():\n",
    "    temp_outputs = cm_transformer(a.to(device), b.to(device), c.to(device))\n",
    "    print(torch.softmax(outputs, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d131cb-92ba-439c-a3ce-f267c0c6508a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa80d8-af2d-49e0-8742-5b777ff3d2a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3b07c-e3d3-47ce-a2c6-e4612e869931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205bf9ba-4002-41b2-8fdb-8a9b2600b830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d256a7e5-a38e-475f-bce1-57934e51584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:06<00:00,  1.27s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:09<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=>retrieved_range=100, MedR=56.5000(4.5000), time=255.9803s\n",
      "Global recall: 1: 0.0020, 5: 0.0170, 10: 0.0250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(56.5, 4.5, {1: 0.002, 5: 0.016999999999999998, 10: 0.025})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank(text_embeddings, image_features, attention_masks, model=cm_transformer, device=device, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f338b76-ae84-47c4-82e8-315c6e48d8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7d1ba-8208-458e-8c69-a92d959f792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|███████████▌                                        | 2695/12148 [16:56<59:31,  2.65it/s]"
     ]
    }
   ],
   "source": [
    "print('Calculating Metrics')\n",
    "image_encoder.eval()\n",
    "text_encoder.eval()\n",
    "cm_transformer.eval()\n",
    "\n",
    "text_embeddings = list()\n",
    "image_features = list()\n",
    "attention_masks = list()\n",
    "with torch.no_grad():\n",
    "    for text, image in tqdm(val_loader):\n",
    "        text_inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "        text_outputs = text_encoder(**text_inputs)\n",
    "        image_outputs = image_encoder(image.to(device))\n",
    "\n",
    "        for text_output, image_feature, attention_mask in zip(text_outputs, image_outputs, text_inputs.attention_mask):\n",
    "            text_embeddings.append(text_output.cpu())\n",
    "            image_features.append(image_feature.cpu())\n",
    "            attention_masks.append(attention_mask.cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stylegan3",
   "language": "python",
   "name": "stylegan3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

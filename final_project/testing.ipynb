{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bededac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from helper import get_transformer_input, save_model, rank\n",
    "from datasets import Recipe1MDataset\n",
    "from models import TextEncoder, ImageEncoder\n",
    "from trainer import train\n",
    "from helper import freeze_params\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4642b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "saved_model_path = 'saved_models/model.pt'\n",
    "saved_weights = torch.load(saved_model_path, map_location='cpu')\n",
    "\n",
    "#transformer_model_path = '/common/home/as3503/as3503/courses/cs536/final_project/final_project/saved_models/1s0qc5ue/model_train_encoders_False_epoch_0.pt'\n",
    "\n",
    "#transformer_weights = torch.load(transformer_model_path, map_location='cpu')\n",
    "device = 'cuda:1'\n",
    "text_encoder = TextEncoder(2, 2)\n",
    "text_encoder.load_state_dict(saved_weights['txt_encoder'])\n",
    "text_encoder = text_encoder.to(device)\n",
    "image_encoder = ImageEncoder()\n",
    "image_encoder.load_state_dict(saved_weights['img_encoder'])\n",
    "image_encoder = image_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8c5e5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Recipe1MDataset\n",
      "In Recipe1MDataset\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Recipe1MDataset(part='train')\n",
    "val_dataset = Recipe1MDataset(part='val')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "save_dir = 'saved_models/'\n",
    "\n",
    "freeze_params(text_encoder)\n",
    "freeze_params(image_encoder)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c51d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        pe = torch.transpose(pe, 0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[0, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, model_dim=768, n_heads=2, n_layers=2, num_image_patches=197, num_classes=2, drop_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.text_pos_embed = SinusoidalPositionalEncoding(model_dim, dropout=drop_rate)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        self.sep_token = nn.Parameter(torch.zeros(1, 1, model_dim))\n",
    "        self.image_pos_embed = nn.Parameter(torch.zeros(1, num_image_patches + 1, model_dim))\n",
    "        self.image_pos_drop = nn.Dropout(p=drop_rate)\n",
    "        layers = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layers, num_layers=n_layers)\n",
    "        self.cls_projection = nn.Linear(model_dim, num_classes)\n",
    "        \n",
    "    def forward(self, image_features, text_features, src_key_padding_mask=None):\n",
    "        batch_size = image_features.shape[0]\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        image_features = torch.cat((cls_token, image_features), dim=1)\n",
    "        image_features = image_features + self.image_pos_embed\n",
    "        image_features = self.image_pos_drop(image_features)\n",
    "        \n",
    "        text_features = self.text_pos_embed(text_features)\n",
    "        \n",
    "        sep_token = self.sep_token.expand(batch_size, -1, -1)\n",
    "        transformer_input = torch.cat((image_features, sep_token, text_features), dim=1)\n",
    "        if src_key_padding_mask is not None:\n",
    "            src_key_padding_mask = torch.cat((torch.zeros(batch_size, image_features.shape[1] + 1).to(transformer_input.device), src_key_padding_mask), dim=1)\n",
    "        transformer_outputs = self.encoder(transformer_input, src_key_padding_mask=src_key_padding_mask)\n",
    "        cls_outputs = transformer_outputs[:, 0, :]\n",
    "        return self.cls_projection(cls_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c63ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45532f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e52a6ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cm_transformer = CrossModalAttention().to(device)\n",
    "\n",
    "cm_transformer = CrossModalAttention()\n",
    "#cm_transformer.load_state_dict(transformer_weights['cm_transformer'])\n",
    "cm_transformer = cm_transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3515ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "train_encoders = False\n",
    "if train_encoders:\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {'params': image_encoder.parameters()},\n",
    "            {'params': text_encoder.parameters()},\n",
    "            {'params': cm_transformer.parameters()}\n",
    "        ],\n",
    "        lr=1e-5\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(cm_transformer.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0e48cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                                                                                | 100/56541 [00:24<3:41:12,  4.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.1208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                | 200/56541 [00:50<3:43:15,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▋                                                                                                                                | 300/56541 [01:15<4:20:26,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▉                                                                                                                                | 401/56541 [01:40<3:32:25,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▏                                                                                                                               | 500/56541 [02:05<3:28:40,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▎                                                                                                                               | 600/56541 [02:31<3:36:15,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▌                                                                                                                               | 700/56541 [03:10<7:34:11,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|█▊                                                                                                                               | 800/56541 [03:52<6:15:09,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██                                                                                                                               | 900/56541 [04:33<6:34:57,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▎                                                                                                                              | 999/56541 [05:12<4:49:42,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.0201\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m save_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcm_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m: cm_transformer\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     31\u001b[0m }\n\u001b[1;32m     32\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_models\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 33\u001b[0m save_model(save_dict, fpath\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_model.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "cm_transformer.train()\n",
    "\n",
    "train_loss, total_samples = 0, 0\n",
    "num_its = 0\n",
    "\n",
    "for text, image in tqdm(train_loader):\n",
    "    \n",
    "    num_its += 1\n",
    "    text_inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    text_outputs = text_encoder(**text_inputs)\n",
    "    image_outputs = image_encoder(image.to(device))\n",
    "    transformer_image_inputs, transformer_text_inputs, output_attention_mask, ground_truth = \\\n",
    "        get_transformer_input(image_outputs, text_outputs, text_inputs.attention_mask)\n",
    "    text_padding_mask = ~output_attention_mask.bool()\n",
    "    outputs = cm_transformer(transformer_image_inputs.to(device), transformer_text_inputs.to(device), text_padding_mask.to(device))\n",
    "    loss = criterion(outputs, ground_truth.to(device).long())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss += loss.item() * image.shape[0]\n",
    "    total_samples += image.shape[0]\n",
    "\n",
    "    if num_its % 100 == 0:\n",
    "        print('Train loss', round(train_loss / total_samples, 4))\n",
    "    \n",
    "    if num_its % 1000 == 0:\n",
    "        print('Saving model')\n",
    "        import os\n",
    "        save_dict = {\n",
    "            'cm_transformer': cm_transformer.state_dict()\n",
    "        }\n",
    "        save_dir = 'saved_models'\n",
    "        save_model(save_dict, fpath=os.path.join(save_dir, f'temp_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "424f4cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/12148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-6.5810,  6.2637],\n",
      "        [-6.3867,  5.9277],\n",
      "        [-5.7426,  5.3994],\n",
      "        [-6.7491,  5.8510],\n",
      "        [-5.2724,  5.1299],\n",
      "        [-5.8812,  5.6835],\n",
      "        [-5.9786,  5.4465],\n",
      "        [-6.5578,  5.7299]], device='cuda:1', grad_fn=<AddmmBackward0>)\n",
      "tensor([[2.6403e-06, 1.0000e+00],\n",
      "        [4.4867e-06, 1.0000e+00],\n",
      "        [1.4491e-05, 9.9999e-01],\n",
      "        [3.3718e-06, 1.0000e+00],\n",
      "        [3.0360e-05, 9.9997e-01],\n",
      "        [9.4950e-06, 9.9999e-01],\n",
      "        [1.0918e-05, 9.9999e-01],\n",
      "        [4.6081e-06, 1.0000e+00]], device='cuda:1', grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_encoder.eval()\n",
    "text_encoder.eval()\n",
    "cm_transformer.eval()\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "val_loss, total_samples = 0, 0\n",
    "for text, image in tqdm(val_loader):\n",
    "    text_inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    text_outputs = text_encoder(**text_inputs)\n",
    "    image_outputs = image_encoder(image.to(device))\n",
    "    transformer_image_inputs, transformer_text_inputs, output_attention_mask, ground_truth = \\\n",
    "        get_transformer_input(image_outputs, text_outputs, text_inputs.attention_mask)\n",
    "    text_padding_mask = ~output_attention_mask.bool()\n",
    "    outputs = cm_transformer(transformer_image_inputs[:8].to(device), transformer_text_inputs[:8].to(device),\n",
    "                             text_padding_mask[:8].to(device))\n",
    "#     loss = criterion(outputs, ground_truth.to(device).long())\n",
    "\n",
    "    print(outputs)\n",
    "    print(softmax(outputs))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ca5eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                             | 0/12148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.7622,  5.5879],\n",
      "        [-6.4880,  6.0461],\n",
      "        [ 5.6285, -5.5787],\n",
      "        [-5.9732,  5.6619],\n",
      "        [-6.4167,  6.1035],\n",
      "        [ 7.0654, -6.5679],\n",
      "        [ 5.2722, -5.1182],\n",
      "        [ 6.6224, -5.9077],\n",
      "        [-6.7211,  5.7733],\n",
      "        [ 5.0049, -5.4297],\n",
      "        [ 6.6561, -6.0565],\n",
      "        [ 5.7452, -5.7584],\n",
      "        [ 6.0133, -5.3118],\n",
      "        [ 7.0897, -6.4577],\n",
      "        [-6.0918,  5.5260],\n",
      "        [ 4.9967, -4.9360],\n",
      "        [ 7.3340, -6.4438],\n",
      "        [ 5.9131, -5.8019],\n",
      "        [ 6.1362, -6.1016],\n",
      "        [ 5.6348, -5.7662],\n",
      "        [-6.8971,  6.0547],\n",
      "        [ 6.6107, -6.1656],\n",
      "        [-5.0777,  4.9768],\n",
      "        [ 6.0133, -5.3118]], device='cuda:1', grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.1768e-05, 9.9999e-01],\n",
      "        [3.6017e-06, 1.0000e+00],\n",
      "        [9.9999e-01, 1.3575e-05],\n",
      "        [8.8499e-06, 9.9999e-01],\n",
      "        [3.6521e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 1.1998e-06],\n",
      "        [9.9997e-01, 3.0725e-05],\n",
      "        [1.0000e+00, 3.6161e-06],\n",
      "        [3.7478e-06, 1.0000e+00],\n",
      "        [9.9997e-01, 2.9396e-05],\n",
      "        [1.0000e+00, 3.0130e-06],\n",
      "        [9.9999e-01, 1.0094e-05],\n",
      "        [9.9999e-01, 1.2066e-05],\n",
      "        [1.0000e+00, 1.3075e-06],\n",
      "        [9.0048e-06, 9.9999e-01],\n",
      "        [9.9995e-01, 4.8557e-05],\n",
      "        [1.0000e+00, 1.0384e-06],\n",
      "        [9.9999e-01, 8.1710e-06],\n",
      "        [1.0000e+00, 4.8439e-06],\n",
      "        [9.9999e-01, 1.1184e-05],\n",
      "        [2.3721e-06, 1.0000e+00],\n",
      "        [1.0000e+00, 2.8270e-06],\n",
      "        [4.2993e-05, 9.9996e-01],\n",
      "        [9.9999e-01, 1.2066e-05]], device='cuda:1', grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_encoder.eval()\n",
    "text_encoder.eval()\n",
    "cm_transformer.eval()\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "val_loss, total_samples = 0, 0\n",
    "for text, image in tqdm(val_loader):\n",
    "    text_inputs = tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n",
    "    text_outputs = text_encoder(**text_inputs)\n",
    "    image_outputs = image_encoder(image.to(device))\n",
    "    transformer_image_inputs, transformer_text_inputs, output_attention_mask, ground_truth = \\\n",
    "        get_transformer_input(image_outputs, text_outputs, text_inputs.attention_mask)\n",
    "    text_padding_mask = ~output_attention_mask.bool()\n",
    "    \n",
    "    indices = torch.randperm(transformer_image_inputs.size()[0])\n",
    "#     outputs = cm_transformer(transformer_image_inputs[indices].to(device), transformer_text_inputs[indices].to(device), text_padding_mask[indices].to(device))\n",
    "        \n",
    "        \n",
    "    outputs = cm_transformer(transformer_image_inputs[indices].to(device), transformer_text_inputs[indices].to(device),\n",
    "                             text_padding_mask[indices].to(device))\n",
    "#     loss = criterion(outputs, ground_truth.to(device).long())\n",
    "\n",
    "    print(outputs)\n",
    "    print(softmax(outputs))\n",
    "    print(ground_truth[indices])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa996cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb73fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f5188",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val_loss = float('inf')\n",
    "project_name = 'cross_modal_attention'\n",
    "wandb.init(project=project_name, entity='cs536')\n",
    "save_dir = os.path.join(save_dir, wandb.run.id)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if train_encoders:\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {'params': image_encoder.parameters()},\n",
    "            {'params': text_encoder.parameters()},\n",
    "            {'params': cm_transformer.parameters()}\n",
    "        ],\n",
    "        lr=lr\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(cm_transformer.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(image_encoder, text_encoder, cm_transformer, train_dataloader, \n",
    "                                tokenizer, criterion, optimizer, train_encoders, device)\n",
    "    val_loss = evaluate(image_encoder, text_encoder, cm_transformer, \n",
    "                        val_dataloader, tokenizer, criterion, device)\n",
    "\n",
    "    # if val_loss < min_val_loss:\n",
    "    min_val_loss = val_loss\n",
    "    if train_encoders:\n",
    "        save_dict = {\n",
    "            'image_encoder': image_encoder.state_dict(),\n",
    "            'text_encoder': text_encoder.state_dict(),\n",
    "            'cm_transformer': cm_transformer.state_dict()\n",
    "        }\n",
    "    else:\n",
    "        save_dict = {\n",
    "            'cm_transformer': cm_transformer.state_dict()\n",
    "        }\n",
    "\n",
    "    save_dict['train_loss'] = train_loss\n",
    "    save_dict['val_loss'] = val_loss\n",
    "\n",
    "    save_model(save_dict, fpath=os.path.join(save_dir, f'model_train_encoders_{train_encoders}_epoch_{epoch}.pt'))\n",
    "\n",
    "    print(f'Epoch: {epoch}, Train loss: {train_loss}, Val loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186576ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross-modal-representation-learning",
   "language": "python",
   "name": "cross-modal-representation-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
